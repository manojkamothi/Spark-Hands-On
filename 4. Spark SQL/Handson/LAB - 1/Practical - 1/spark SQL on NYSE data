import spark.sqlContext.implicits._

case class nyse(exchangename:String, stockid:String, date:String, open:Double, high:Double, low:Double, close:Double, volume:Long, adj_close:Double) ; 


val nyse1 = sc.textFile("/home/hduser/NYSE.csv")

val nyse2 = nyse1.map(_.split(","))

val nyse3 = nyse2.map(v => nyse(v(0).toString.trim, v(1).toString.trim, v(2).toString, v(3).toDouble, v(4).toDouble,v(5).toDouble, v(6).toDouble,v(7).toLong,v(8).toDouble))

val nyse4 = nyse3.toDF

nyse4.registerTempTable("sp_nyse");

to find total volume for Top 5 stocks
-----------------------------------

val top5 = spark.sql("select stockid, sum(volume) as total from sp_nyse group by stockid order by total desc LIMIT 5")

sending the output to hive table
--------------------------------
top5.write.mode("overwrite").saveAsTable("retail.top5");


in hive
--------
create database retail



top5.show()
top5.printSchema()
top5.write.csv("/home/hduser/intel/spark2");



to find the max variance for each stock (order it on max variance. show top 10 records)
---------------------------------------------------------------------------------------

val top10 = spark.sql("select stockid, date, round(max((high-low)/low*100),2) as maxvar from sp_nyse group by stockid, date order by maxvar desc LIMIT 10")

top10.write.csv("hdfs://localhost:54310/intel/spark-maxvar");

top10.collect.foreach(println)


 


