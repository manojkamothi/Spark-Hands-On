//Broad Cast Variables 

//Broadcast variables − used to efficiently, distribute large values.

val broadcastVar = sc.broadcast(Array(1, 2, 3))
broadcastVar.value
--------------------------------------------------------
case class Employee(name:String, age:Int, depId: String)
case class Department(id: String, name: String)
 
val employeesRDD = sc.parallelize(Seq( 
    Employee("Mary", 33, "IT"), 
    Employee("Paul", 45, "IT"), 
    Employee("Peter", 26, "MKT"), 
    Employee("Jon", 34, "MKT"), 
    Employee("Sarah", 29, "IT"),
    Employee("Steve", 21, "Intern")
))
val departmentsRDD = sc.parallelize(Seq( 
    Department("IT", "IT  Department"),
    Department("MKT", "Marketing Department"),
    Department("FIN", "Finance & Controlling")
))
 
val employeesDF = employeesRDD.toDF
val departmentsDF = departmentsRDD.toDF
// The DataFrame API of Spark makes it very concise to create a broadcast variable out of the department DataFrame.

 
// materializing the department data
val tmpDepartments = broadcast(departmentsDF.as("departments"))
 
 // join by employees.depID == departments.id
employeesDF.join(broadcast(tmpDepartments), 
   $"depId" === $"id",   
   "inner").show()

-------------------------------------------------------------------

//We are broadcasting us_states data so that every node will get same data for joining
Tables instead of shuffling data between nodes in the cluster.

val localDF = sqlContext.read.json("/user/cloudera/datasets/us_states.json")
val broadcastStateData = sqlContext.sparkContext.broadcast(localDF.collectAsList())
val broadcastSchema = sqlContext.sparkContext.broadcast(localDF.schema)

// Create a DataFrame based on the store locations.
val storesDF = sqlContext.read.json("/user/cloudera/datasets/store_locations.json")

// Create a DataFrame of US state data with the broadcast variables.

val stateDF = sqlContext.createDataFrame(broadcastStateData.value, broadcastSchema.value)
// Join the DataFrames to get an aggregate count of stores in each US Region
//"How many stores are in each US region?"

val joinedDF = storesDF.join(stateDF, "state").groupBy("census_region").count()
joinedDF.show()
===========================================================================================
//Accumulators

val accum=sc.accumulator(0)
sc.parallelize(Array(1,2,3,4)).foreach(x =>accum+= x)

//WordCount with Accumulators

val accum=sc.accumulator(0)
val wordcount = sc.textFile("/user/cloudera/datasets/CHANGES.txt").flatMap(_.split(" ")).foreach(W => accum += W.length)
println(accum.value)

=====================================================================================================


     val payRDD = sc.textFile("userInfo.csv")
     val payPair = payRDD.map(x => (x.split(",")(0),x))

     val usrRDD = sc.textFile("payments.csv")
     val usrPair = usrRDD.map(
       x => {
         if(x.split(",")(1)=="Delhi") {
           purchaseInDelhi += 1
         }
       (x.split(",")(0),x.split(",")(2))
       }
     )

     val usrMap = usrPair.collectAsMap()
     val r = payPair.map(v => (v._1,(usrMap(v._1),v._2)))

     r.foreach(println)



