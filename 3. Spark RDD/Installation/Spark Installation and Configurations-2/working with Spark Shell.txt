Spark is having 3 interactive Shell modes 

1. spark-shell (this will open spark-shell with scala support)
2. pyspark	(this will open spark-shell with python support)
3. sparkR  (this will open spark-shell with R support)

$spark-shell
Spark context Web UI available at http://10.1.7.49:4040
Spark context available as 'sc' (master = local[*], app id = local-1493028761058).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.0.2
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)
Type in expressions to have them evaluated.
Type :help for more information.

scala> sc
res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@5d5d3a5c

scala> spark
res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6b3d9c38

scala>

/**************************************************************************/


$pyspark 

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.0.2
      /_/

Using Python version 2.7.13 (default, Dec 20 2016 23:09:15)
SparkSession available as 'spark'.
>>> sc
<pyspark.context.SparkContext object at 0x7fc74bd646d0>
>>> spark
<pyspark.sql.session.SparkSession object at 0x7fc74b7c9810>
>>>

/***********************************************************/

$sparkR

 Welcome to
    ____              __
   / __/__  ___ _____/ /__
  _\ \/ _ \/ _ `/ __/  '_/
 /___/ .__/\_,_/_/ /_/\_\   version  2.0.2
    /_/


 SparkSession available as 'spark'.
> sc
Java ref type org.apache.spark.api.java.JavaSparkContext id 2
> spark
Java ref type org.apache.spark.sql.SparkSession id 1
>


/********************************************************************/

in any shell 

sc :sparkContext (The driver for Spark Program)
spark : sparkSession (The sparkSql object to deal with Dataframes and Datasets)


/**************************************************************/

Spark shell arguments:-

  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.
  --deploy-mode DEPLOY_MODE   client,cluster (default client)
  
  --class CLASS_NAME          Your application's main class (for Java / Scala apps).
  --name NAME                 A name of your application.
  --jars JARS                 Comma-separated list of local jars to include on the driver
                              and executor classpaths.
  --packages                  Comma-separated list of maven coordinates of jars to include
                              on the driver and executor classpaths. The format for the
                              coordinates should be groupId:artifactId:version.
							  
  --repositories              Comma-separated list of additional remote repositories to
                              search for the maven coordinates given with --packages.
  --files FILES               Comma-separated list of files to be placed in the working
                              directory of each executor.

  --conf PROP=VALUE           Arbitrary Spark configuration property.
  --properties-file FILE      Path to a file from which to load extra properties. If not
                              specified, this will look for conf/spark-defaults.conf.

  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).
    
  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).

  --version,                  Print the version of current Spark.
  
  
  Example :-
  
spark-shell --master local[2] --name MyApp1 
spark-shell --master yarn --deploy-mode client --name MyApp2   --conf spark.ui.port=4041 
spark-shell --master yarn --deploy-mode client --name MyApp2   --conf spark.ui.port=4041 --driver-memory 2G --executor-memory 2G --packages mysql:mysql-connector-java:5.1.30 
  
  


