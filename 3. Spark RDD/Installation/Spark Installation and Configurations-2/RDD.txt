//Resilient Distributed Datasets:-

we can create RDD using two ways 
1. sparkContext.parallelize 
2. sparkContext.textFile

//Method one :-

scala> val numbers = List(1,2,3,4,5)
numbers: List[Int] = List(1, 2, 3, 4, 5)

scala> val rdd = sc.parallelize(numbers)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:26

scala> val namesRdd = sc.parallelize(Seq("joe","john","adom"))
namesRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[2] at parallelize at <console>:24

scala> val namesRdd = sc.parallelize(Seq("joe","john","adom"),2)
namesRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:24

scala> namesRdd.foreach(println)



//Method two:-

scala> sc.textFile("/user/emp.csv")
res1: org.apache.spark.rdd.RDD[String] = /user/emp.csv MapPartitionsRDD[5] at textFile at <console>:25

Note :- by default spark will take hadoop as its file system so if you are not mentioning the protocal it will look for hadoop directory.
if you want to read local file you need to use file:// , for accessing hdfs file hdfs://<host>:<port>/ , for amazon s3 s3:// 

scala> res1.count
res2: Long = 4

scala> res1.foreach(println)
siva,sales,30000
ramu,marketing,34000
devi,marketing,60000
john,sales,45000


//using makeRDD method :-


scala> sc.makeRDD(1 to 100,3)
res3: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at makeRDD at <console>:16


//functional programming

def printline(line:String){println(line)}


scala> sc.textFile("/user/emp.csv")
res1: org.apache.spark.rdd.RDD[String] = /user/emp.csv MapPartitionsRDD[5] at textFile at <console>:25

res1.foreach(printline)
[Stage 3:>                                                          (0 + 2) / 2]
devi,marketing,60000
siva,sales,30000
ramu,marketing,34000
john,sales,45000






