//How to create sparkContext object in spark-shell 

//The shell already creates one sparkContext object for the developer so you need to stop it first.

scala> sc.stop


//import required classes

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

//must have these two parameters.
val conf = new SparkConf().setMaster("local").setAppName("MyApp")

//optional
conf.set("spark.ui.port","4041")

//create sparkContext using SparkConf

val sc = new SparkContext(conf)

//sparkDriver has instantiated with local as master

//how to create sparkContext with master as yarn and deploymode client


val conf = new SparkConf().setMaster("yarn").setAppName("MyApp")

conf.set("spark.submit.deployMode","client")


//create sparkContext using SparkConf

val sc = new SparkContext(conf)

