Spark RDD Transformations:-

1)map
2)flatMap
3)filter
4)mapPartitions
5)mapPartitionsWithIndex
6)sample
7)union
8)intersection
9)distinct
10)groupBy
11)keyBy
12)Zip
13)zipwithIndex
14)Colease
15)Repartition
16)sortBy
/************************************************************************************************/

//map(func)

val x = sc.parallelize(List("spark", "rdd", "example",  "sample", "example"), 3)
val y = x.map(x => (x,1))
y.collect

// rdd y can be re writen with shorter syntax in scala as 
scala>valy = x.map((_, 1))
scala>y.collect

val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2)
val b = a.map(x => (x.length, x))
b.values.collect
b.keys.collect

/************************************************************************************************/
//flatMap(func)
//"Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item)."

example 1 :-

scala> val data = sc.parallelize(List(1,2,3)).flatMap(x => List(x,x,x)).collect
data: Array[Int] = Array(1, 1, 1, 2, 2, 2, 3, 3, 3)

scala> val data = sc.parallelize(List(1,2,3)).map(x => List(x,x,x)).collect
data: Array[List[Int]] = Array(List(1, 1, 1), List(2, 2, 2), List(3, 3, 3))

example 2:-

scala> val names = sc.parallelize(List("joe admon john smith mith bob")).map(_.split(" ")).collect
names: Array[Array[String]] = Array(Array(joe, admon, john, smith, mith, bob))

scala> val names = sc.parallelize(List("joe admon john smith mith bob")).flatMap(_.split(" ")).collect
names: Array[String] = Array(joe, admon, john, smith, mith, bob)


/************************************************************************************************/

//filter(func) :-

//Similar to map, but each input item can be mapped to 0 or more output items 

//Note that the filter operation does not mutate the existing inputRDD. Instead, it returns
a pointer to an entirely new RDD. inputRDDcan still be re-used later in the program


scala> val names = sc.parallelize(List("Spark","Hadoop","MapReduce","Pig","Spark2","Hadoop2","Pig")).filter(_.equals("Spark")).collect
names: Array[String] = Array(Spark)

scala> val names = sc.parallelize(List("Spark","Hadoop","MapReduce","Pig","Spark2","Hadoop2","Pig")).filter(_.contains("Spark")).collect
names: Array[String] = Array(Spark, Spark2)

scala> val names = sc.parallelize(List((4,"Spark"),(3,"Hadoop"),(5,"MapReduce"),(2,"Pig"),(1,"Spark2"),(10,"Hadoop2"),(8,"Pig"))).filter(_._2.contains("Spark")).filter(_._1<2).collect
names: Array[(Int, String)] = Array((4,Spark), (1,Spark2))

scala> val names = sc.parallelize(List((4,"Spark"),(3,"Hadoop"),(5,"MapReduce"),(2,"Pig"),(1,"Spark2"),(10,"Hadoop2"),(8,"Pig"))).filter(_._2.contains("Spark")).filter(_._1<2).collect
names: Array[(Int, String)] = Array((1,Spark2))

scala> val names = sc.parallelize(List((4,"Spark"),(3,"Hadoop"),(5,"MapReduce"),(2,"Pig"),(1,"Spark2"),(10,"Hadoop2"),(8,"Pig")))
scala> names.sortByKey().filterByRange(1,4).collect
res8: Array[(Int, String)] = Array((1,Spark2), (2,Pig), (3,Hadoop), (4,Spark)


/************************************************************************************************/


//sample(withreplacement:boolean,fraction:Double,seed:Long)

//when you are taking sample from RDD 
//withReplacement - can elements be sampled multiple times (replaced when sampled out)
//fraction - expected size of the sample as a fraction of this RDD's size 
	without replacement: probability that each element is chosen; 	
	fraction must be [0, 1] with replacement: expected number of times each element is chosen; fraction must be >= 0
//seed - seed for the random number generator

scala>val parallel = sc.parallelize(1 to 9)
parallel: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[470] at parallelize at <console>:12
 
scala>parallel.sample(true,.2).count
res403: Long = 3
 
scala>parallel.sample(true,.2).count
res404: Long = 2

//parallel.sample(true,.2).doesn't return the same sample size: that's because spark internally uses something called Bernoulli sampling for taking the sample. 

scala> val data = sc.parallelize(1 to 10)
data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[72] at parallelize at <console>:24

scala> val s = data.sample(true,.4,4)
s: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[76] at sample at <console>:26

scala> s.collect
res69: Array[Int] = Array(2, 3, 4, 4, 9)

//seed  guarantee is that if you start from the same seed you will get the same sequence of numbers

scala> val s = data.sample(true,.4,4)
s: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[77] at sample at <console>:26

scala> s.collect
res70: Array[Int] = Array(2, 3, 4, 4, 9)

/************************************************************************************************/

//union(a different rdd)


scala>val parallel = sc.parallelize(1 to 9)
parallel: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[477] at parallelize at <console>:12
 
scala>val par2 = sc.parallelize(5 to 15)
par2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[478] at parallelize at <console>:12
 
scala>parallel.union(par2).collect
res408: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)


/************************************************************************************************/

//intersection(a different rdd)

scala> val parallel = sc.parallelize(1 to 9)
parallel: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[81] at parallelize at <console>:24

scala> val par2 = sc.parallelize(5 to 15)
par2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[82] at parallelize at <console>:24

scala> parallel.intersection(par2).collect
res72: Array[Int] = Array(6, 7, 8, 9, 5)


/************************************************************************************************/

//distinct([numTasks])

scala> val parallel = sc.parallelize(1 to 9)
parallel: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[81] at parallelize at <console>:24

scala> val par2 = sc.parallelize(5 to 15)
par2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[82] at parallelize at <console>:24

scala> parallel.union(par2).distinct().collect
res74: Array[Int] = Array(12, 13, 1, 14, 2, 15, 3, 4, 5, 6, 7, 8, 9, 10, 11)



/************************************************************************************************/

//sortBy(func)

//Return this RDD sorted by the given key function

scala> val y = sc.parallelize(Array(5, 7, 1, 3, 2, 1))
y: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[103] at parallelize at <console>:24

scala> y.sortBy(c => c,false).collect
res82: Array[Int] = Array(7, 5, 3, 2, 1, 1)

scala> y.sortBy(c => c,true).collect
res83: Array[Int] = Array(1, 1, 2, 3, 5, 7

scala> val z = sc.parallelize(Array(("H", 10), ("A", 26), ("Z", 1), ("L", 5)))
z: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[119] at parallelize at <console>:24

scala> z.sortBy(c => c._1, true).collect
res84: Array[(String, Int)] = Array((A,26), (H,10), (L,5), (Z,1))

scala> z.sortBy(c => c._2, true).collect
res85: Array[(String, Int)] = Array((Z,1), (L,5), (H,10), (A,26))


/************************************************************************************************/

//mapPartitions(func)

mapPartitions() can be used as an alternative to map() & foreach().mapPartitions() is called once for each Partition unlike map() & foreach() which is called for each element in the RDD.

scala> var parellal = sc.parallelize(1 to 9, 3)
parellal: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[130] at parallelize at <console>:24

scala> parellal.mapPartitions(x=>List(x.next).iterator).collect
res86: Array[Int] = Array(1, 4, 7)

scala> var parellal = sc.parallelize(1 to 9)
parellal: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[132] at parallelize at <console>:24

scala> parellal.mapPartitions(x=>List(x.next).iterator).collect
res87: Array[Int] = Array(1, 2, 4, 5, 7, 8)

/************************************************************************************************/

//mapPartitionsWithIndex

Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.

scala> val nums = sc.parallelize(List(1,2,3,4,5,6), 2)
nums: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[139] at parallelize at <console>:24

scala> def myfunc(index: Int, iter: Iterator[(Int)]) : Iterator[String] = {
      iter.toList.map(x => "[index:" +  index + ", val: " + x + "]").iterator
      }
myfunc: (index: Int, iter: Iterator[Int])Iterator[String]

scala> nums.mapPartitionsWithIndex(myfunc).collect
res93: Array[String] = Array([index:0, val: 1], [index:0, val: 2], [index:0, val: 3], [index:1, val: 4], [index:1, val: 5], [index:1, val: 6])


/******************************************************************************************************************/

//groupBy , keyBy

scala> val rdd = sc.parallelize(List((1,"apple"),(2,"banana"),(3,"lemon"),(4,"apple"),(1,"lemon")) )
rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[0] at parallelize at <console>:27

scala> rdd.groupBy(_._1)
res7: org.apache.spark.rdd.RDD[(Int, Iterable[(Int, String)])] = ShuffledRDD[9] at groupBy at <console>:30

scala> rdd.groupBy(_._1).collect
res8: Array[(Int, Iterable[(Int, String)])] = Array((4,CompactBuffer((4,apple))), (2,CompactBuffer((2,banana))), (1,CompactBuffer((1,apple), (1,lemon))), (3,CompactBuffer((3,lemon))))

scala> rdd.keyBy(_._1)
res9: org.apache.spark.rdd.RDD[(Int, (Int, String))] = MapPartitionsRDD[12] at keyBy at <console>:30

scala> rdd.keyBy(_._1).collect
res10: Array[(Int, (Int, String))] = Array((1,(1,apple)), (2,(2,banana)), (3,(3,lemon)), (4,(4,apple)), (1,(1,lemon)))

scala> rdd.keyBy(_._2).collect
res11: Array[(String, (Int, String))] = Array((apple,(1,apple)), (banana,(2,banana)), (lemon,(3,lemon)), (apple,(4,apple)), (lemon,(1,lemon)))

/**********************************************************************************************************************/

// zip, zipwithIndex


//zip: Returns a list formed from this list and another iterable collection by combining 
corresponding elements in pairs. If one of the two collections is longer than the 
other, its remaining elements are ignored.


//zipWithIndex : Zips this list with its indices.

Returns: A new list containing pairs consisting of all elements of this list 
paired with their index. Indices start at 0.


scala> val odds = sc.parallelize(List(1,3,5))
odds: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at <console>:27

scala> val evens = sc.parallelize(List(2,4,6))
evens: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[18] at parallelize at <console>:27

scala> val numbers = odds zip evens
numbers: org.apache.spark.rdd.RDD[(Int, Int)] = ZippedPartitionsRDD2[19] at zip at <console>:31

scala> numbers.collect
res12: Array[(Int, Int)] = Array((1,2), (3,4), (5,6))

scala> val evenswithIndex = evens zipWithIndex
evenswithIndex: org.apache.spark.rdd.RDD[(Int, Long)] = ZippedWithIndexRDD[20] at zipWithIndex at <console>:29

scala> evenswithIndex.collect
res13: Array[(Int, Long)] = Array((2,0), (4,1), (6,2))

/********************************************************************************************************/

colease
repartition

scala> val numbers = sc.parallelize( 1 to 100,2)
numbers: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[4] at parallelize at <console>:27

scala> numbers.partitions.size
res4: Int = 2

scala> numbers.foreachPartition(x => println(x.toList))
List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50)
List(51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)

scala> val numberIncreased = numbers.repartition(10)
numberIncreased: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[9] at repartition at <console>:29

scala> numberIncreased.partitions.size
res7: Int = 10

scala> numberIncreased.foreachPartition(x => println(x.toList))
List(3, 13, 23, 33, 43, 58, 68, 78, 88, 98)
List(8, 18, 28, 38, 48, 53, 63, 73, 83, 93)
List(9, 19, 29, 39, 49, 54, 64, 74, 84, 94)
List(1, 11, 21, 31, 41, 56, 66, 76, 86, 96)
List(10, 20, 30, 40, 50, 55, 65, 75, 85, 95)
List(5, 15, 25, 35, 45, 60, 70, 80, 90, 100)
List(6, 16, 26, 36, 46, 51, 61, 71, 81, 91)
List(2, 12, 22, 32, 42, 57, 67, 77, 87, 97)
List(7, 17, 27, 37, 47, 52, 62, 72, 82, 92)
List(4, 14, 24, 34, 44, 59, 69, 79, 89, 99)

scala> numberIncreased.coalesce(3)
res11: org.apache.spark.rdd.RDD[Int] = CoalescedRDD[10] at coalesce at <console>:32

scala> res11.foreachPartition(x => println(x.toList))
List(2, 12, 22, 32, 42, 57, 67, 77, 87, 97, 4, 14, 24, 34, 44, 59, 69, 79, 89, 99, 7, 17, 27, 37, 47, 52, 62, 72, 82, 92)
List(3, 13, 23, 33, 43, 58, 68, 78, 88, 98, 5, 15, 25, 35, 45, 60, 70, 80, 90, 100, 8, 18, 28, 38, 48, 53, 63, 73, 83, 93)
List(10, 20, 30, 40, 50, 55, 65, 75, 85, 95, 1, 11, 21, 31, 41, 56, 66, 76, 86, 96, 6, 16, 26, 36, 46, 51, 61, 71, 81, 91, 9, 19, 29, 39, 49, 54, 64, 74, 84, 94)
