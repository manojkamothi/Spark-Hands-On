//Unlike Transformations which produce RDDs, action functions produce a value back to the Spark driver program.  Actions may trigger a previously constructed, lazy RDD to be evaluated.

/***********************************************************************************/

//reduce(func)

Aggregate the elements of a dataset through fu

scala> val a = sc.parallelize(1 to 10, 3)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[141] at parallelize at <console>:24

scala> a.reduce(_+_)
res94: Int = 55

scala> val names1 = sc.parallelize(List("abe", "abby", "apple"))
names1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[142] at parallelize at <console>:24

scala> names1.flatMap(k => List(k.size) ).reduce((t1,t2) => t1 + t2)
res96: Int = 12

scala> names1.reduce((t1,t2) => t1 + t2)
res97: String = abeabbyapple

scala> val names2 = sc.parallelize(List("apple", "beatty", "beatrice")).map(a => (a, a.size))
names2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[145] at map at <console>:26

scala> names2.flatMap(t => Array(t._2)).reduce(_ + _)
res98: Int = 19


/***********************************************************************************/



//first()

Return the first element in the RDD

scala> val names2 = sc.parallelize(List("apple", "beatty", "beatrice"))
names2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[159] at parallelize at <console>:24

scala> names2.first
res107: String = apple
/********************************************************************************************/

//takeOrdered


It will return a Array with given numbers of ordered values in the  RDD

Ex:
scala> val nums = sc.parallelize(List(1,5,3,9,4,0,2))
nums: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[9] at parallelize at <console>:30

scala> nums.takeOrdered(4)
res19: Array[Int] = Array(0, 1, 2, 3)


/***********************************************************************************/

//take(n)

It will return a Array with first  given numbers of values in the  RDD

Ex:
scala> val nums = sc.parallelize(List(1,5,3,9,4,0,2))
nums: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[9] at parallelize at <console>:30

scala> nums.take(4)
res20: Array[Int] = Array(1, 5, 3, 9)

/***********************************************************************************/

//count()

It will return a Long value that indicates how many elements presented in the RDD.
Ex:
scala> val nums = sc.parallelize(List(1,5,3,9,4,0,2))
nums: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[9] at parallelize at <console>:30

scala> nums.count
res21: Long = 7


scala> val c = sc.parallelize(List("Gnu", "Cat", "Rat", "Dog"), 2)
c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[151] at parallelize at <console>:24

scala> c.count
res103: Long = 4

scala> val c = sc.parallelize(List((3, "Gnu"), (3, "Yak"), (5, "Mouse"), (3, "Dog")), 2)
c: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[152] at parallelize at <console>:24

scala> c.countByKey
res104: scala.collection.Map[Int,Long] = Map(3 -> 3, 5 -> 1)

scala> val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))
b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[155] at parallelize at <console>:24

scala> b.countByValue
res105: scala.collection.Map[Int,Long] = Map(5 -> 1, 1 -> 6, 6 -> 1, 2 -> 3, 7 -> 1, 3 -> 1, 8 -> 1, 4 -> 2)


/************************************************************************************************************************/
//collect(func)

collect returns the elements of the dataset as an array back to the driver program.
collect is often used in previously provided examples such as Spark Transformation Examples in order to show the values of the return.

scala> val c = sc.parallelize(List("Gnu", "Cat", "Rat", "Dog", "Gnu", "Rat"), 2)
c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[148] at parallelize at <console>:24

scala> c.collect
res100: Array[String] = Array(Gnu, Cat, Rat, Dog, Gnu, Rat)


scala> val a = sc.parallelize(List(1, 2, 1, 3), 1)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[149] at parallelize at <console>:24

scala> val b = a.zip(a)
b: org.apache.spark.rdd.RDD[(Int, Int)] = ZippedPartitionsRDD2[150] at zip at <console>:26

scala> b.collect
res101: Array[(Int, Int)] = Array((1,1), (2,2), (1,1), (3,3))

scala> b.collectAsMap
res102: scala.collection.Map[Int,Int] = Map(2 -> 2, 1 -> 1, 3 -> 3)


/***********************************************************************************************/
//collectAsMap


It will return A Map object containg all key value pairs converted as Map.

Ex:
scala> val alphanumerics = sc.parallelize(List((1,"a"),(2,"b"),(3,"c")))
alphanumerics: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[11] at parallelize at <console>:30

scala> alphanumerics.collectAsMap
res22: scala.collection.Map[Int,String] = Map(2 -> b, 1 -> a, 3 -> c)


/***********************************************************************************/
//saveAsTextFile(filepath)

Write out the elements of the data set as a text file in a filepath directory on the filesystem, HDFS or any other Hadoop-supported file system.

Ex:
val a = sc.parallelize(1 to 10000, 3)
a.saveAsTextFile("mydata_a")
val x = sc.parallelize(List(1,2,3,4,5,6,6,7,9,8,10,21), 3)
x.saveAsTextFile("hdfs://localhost:8020/user/hadoop/test");




/***********************************************************************************************/
//foreachPartition

//Executes an parameterless function for each partition. Access to the data items contained in the partition is provided via the iterator argument

scala> val b = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)
b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[147] at parallelize at <console>:24

scala> b.foreachPartition(x => println(x.reduce(_ + _)))
6
15
24

/***********************************************************************************/

//foreach

It will pass each element in the RDD for given function .

EX:
scala> val alphanumerics = sc.parallelize(List((1,"a"),(2,"b"),(3,"c")))
alphanumerics: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[15] at parallelize at <console>:27

scala> alphanumerics.foreach(println)
(2,b)
(1,a)
(3,c)



/********************************************************************************************/
//top(n)

scala> val c = sc.parallelize(Array(6, 9, 4, 7, 5, 8), 2)
c: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[160] at parallelize at <console>:24

scala> c.top(2)
res108: Array[Int] = Array(9, 8)

/**************************************************************************************************/

//Max,Min,Sum,Mean,Variance,stdev


Spark RDD supports some Mathametical Actions like below.

Ex:
scala> val numbers = sc.parallelize(1 to 100)
numbers: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at parallelize at <console>:27

scala> numbers.sum
res21: Double = 5050.0

scala> numbers.max
res22: Int = 100

scala> numbers.min
res23: Int = 1

scala> numbers.mean
res24: Double = 50.5

scala> numbers.variance
res25: Double = 833.25

scala> numbers.stdev
res26: Double = 28.86607004772212







